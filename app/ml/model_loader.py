import re
import onnxruntime as ort
import pickle
from pathlib import Path
from typing import Optional, Dict, Tuple
from app.config import settings
import logging

logger = logging.getLogger(__name__)

# S3 íŒŒì¼ëª… íŒ¨í„´  (ì˜ˆ: 60d_20260206.onnx, 60d_preprocessing_20260206.pkl)
_ONNX_PATTERN = re.compile(r"60d_(\d{8})\.onnx$")
_PKL_PATTERN = re.compile(r"60d_preprocessing_(\d{8})\.pkl$")


class ONNXModelLoader:
    """
    ONNX ëª¨ë¸ ë¡œë”

    ë‘ ê°€ì§€ ëª¨ë“œ ì§€ì›:
    - local : LOCAL_MODEL_PATH í´ë”ì—ì„œ *.onnx / *.pkl ì§ì ‘ ë¡œë“œ
    - s3   : S3 prefix ì•„ë˜ì—ì„œ ë‚ ì§œ(YYYYMMDD) ê¸°ì¤€ ìµœì‹  íŒŒì¼ì„ ì°¾ì•„ ë‹¤ìš´ë¡œë“œ
              ETag ê¸°ë°˜ìœ¼ë¡œ ë³€ê²½ ê°ì§€ â†’ ìë™ ë¦¬ë¡œë“œ

    S3 í‚¤ ì˜ˆì‹œ:
      s3://aitech-storage/models/enhanced_tft/champion/60d_20260206.onnx
      s3://aitech-storage/models/enhanced_tft/champion/60d_preprocessing_20260206.pkl
    """

    def __init__(self):
        self.mode = settings.model_load_mode
        self.local_path = Path(settings.local_model_path)

        # ìºì‹œ -----------------------------------------------------------------
        self.sessions: Dict[str, ort.InferenceSession] = {}      # {commodity: session}
        self.preprocessing_info: Dict[str, dict] = {}             # {commodity: pkl_data}
        self._loaded_keys: Dict[str, Dict[str, str]] = {}         # {commodity: {"onnx_key":..., "pkl_key":...}}
        self._etags: Dict[str, Dict[str, str]] = {}               # {commodity: {"model": etag, "pkl": etag}}

        # S3 í´ë¼ì´ì–¸íŠ¸ (lazy init)
        self._s3_client = None
        # ë¡œì»¬ ìºì‹œ ë””ë ‰í† ë¦¬ (S3 ë‹¤ìš´ë¡œë“œìš©)
        self._cache_dir = Path("./models_cache")

        logger.info(f"ëª¨ë¸ ë¡œë” ì´ˆê¸°í™”: mode={self.mode}, path={self.local_path}")

    # ===========================================
    # Public API
    # ===========================================

    def load_session(self, commodity: str = "corn") -> ort.InferenceSession:
        """
        ONNX ì„¸ì…˜ ë¡œë“œ (ìºì‹±)

        - local ëª¨ë“œ: temp/ í´ë”ì—ì„œ íŒŒì¼ì„ ì°¾ì•„ ë¡œë“œ
        - s3 ëª¨ë“œ  : S3 prefix ì•„ë˜ ìµœì‹  íŒŒì¼ì„ ë‹¤ìš´ë¡œë“œ í›„ ë¡œë“œ
        """
        if self.mode == "local":
            return self._load_local(commodity)
        return self._load_from_s3(commodity)

    def get_preprocessing_info(self, commodity: str = "corn") -> dict:
        """ì „ì²˜ë¦¬ ì •ë³´(pkl) ë°˜í™˜. ì„¸ì…˜ì´ ì—†ìœ¼ë©´ ë¨¼ì € ë¡œë“œ."""
        if commodity not in self.sessions:
            self.load_session(commodity)
        return self.preprocessing_info.get(commodity, {})

    def check_and_update(self, commodity: str = "corn") -> bool:
        """
        S3ì— ìƒˆ ëª¨ë¸ì´ ì˜¬ë¼ì™”ëŠ”ì§€ í™•ì¸ â†’ ë³€ê²½ëìœ¼ë©´ ë¦¬ë¡œë“œ.

        Returns:
            True = ëª¨ë¸ ê°±ì‹ ë¨, False = ë³€ê²½ ì—†ìŒ
        """
        if self.mode != "s3":
            logger.debug("ë¡œì»¬ ëª¨ë“œì—ì„œëŠ” ìë™ ì—…ë°ì´íŠ¸ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
            return False

        s3 = self._get_s3_client()
        bucket = settings.model_s3_bucket

        # ìµœì‹  íŒŒì¼ í‚¤ ì¡°íšŒ
        latest_onnx_key, latest_pkl_key = self._find_latest_s3_keys(s3, bucket)
        if not latest_onnx_key:
            logger.warning("S3ì—ì„œ ONNX íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return False

        # í˜„ì¬ ë¡œë“œëœ í‚¤ì™€ ë¹„êµ
        loaded = self._loaded_keys.get(commodity, {})
        if loaded.get("onnx_key") == latest_onnx_key:
            logger.info(f"[{commodity}] ëª¨ë¸ ë³€ê²½ ì—†ìŒ: {latest_onnx_key}")
            return False

        logger.info(
            f"[{commodity}] ìƒˆ ëª¨ë¸ ê°ì§€! "
            f"({loaded.get('onnx_key', 'None')} â†’ {latest_onnx_key})"
        )

        # ê¸°ì¡´ ì„¸ì…˜ ì œê±° í›„ ì¬ë¡œë“œ
        self.sessions.pop(commodity, None)
        self.preprocessing_info.pop(commodity, None)
        self._etags.pop(commodity, None)
        self._loaded_keys.pop(commodity, None)
        self._load_from_s3(commodity)
        return True

    # ===========================================
    # Local ëª¨ë“œ
    # ===========================================

    def _load_local(self, commodity: str) -> ort.InferenceSession:
        """ë¡œì»¬ íŒŒì¼ì—ì„œ ëª¨ë¸ ë¡œë“œ"""
        if commodity in self.sessions:
            return self.sessions[commodity]

        onnx_file, pkl_file = self._find_local_files()

        logger.info(f"ONNX ì„¸ì…˜ ìƒì„± ì¤‘... {onnx_file.name}")
        session = ort.InferenceSession(
            str(onnx_file), providers=["CPUExecutionProvider"]
        )
        self.sessions[commodity] = session
        logger.info(f"âœ… ONNX ì„¸ì…˜ ìƒì„± ì™„ë£Œ: {onnx_file.name}")

        if pkl_file and pkl_file.exists():
            with open(pkl_file, "rb") as f:
                self.preprocessing_info[commodity] = pickle.load(f)
            logger.info(f"âœ… ì „ì²˜ë¦¬ ì •ë³´ ë¡œë“œ ì™„ë£Œ: {pkl_file.name}")
        else:
            logger.warning("âš ï¸ ì „ì²˜ë¦¬ ì •ë³´ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤")

        return session

    def _find_local_files(self) -> Tuple[Path, Optional[Path]]:
        """ë¡œì»¬ ê²½ë¡œì—ì„œ ONNX, PKL íŒŒì¼ ì°¾ê¸° (íŒŒì¼ëª… ì •ë ¬ â†’ ë§ˆì§€ë§‰ = ìµœì‹ )"""
        onnx_files = sorted(self.local_path.glob("*.onnx"))
        pkl_files = sorted(self.local_path.glob("*.pkl"))

        if not onnx_files:
            raise FileNotFoundError(
                f"ONNX ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {self.local_path}"
            )

        onnx_file = onnx_files[-1]
        pkl_file = pkl_files[-1] if pkl_files else None

        logger.info(f"ë¡œì»¬ ëª¨ë¸ íŒŒì¼ ë°œê²¬: {onnx_file.name}")
        if pkl_file:
            logger.info(f"ì „ì²˜ë¦¬ ì •ë³´ íŒŒì¼ ë°œê²¬: {pkl_file.name}")

        return onnx_file, pkl_file

    # ===========================================
    # S3 ëª¨ë“œ
    # ===========================================

    def _load_from_s3(self, commodity: str) -> ort.InferenceSession:
        """S3ì—ì„œ ìµœì‹  ëª¨ë¸ì„ ë‹¤ìš´ë¡œë“œí•˜ê³  ì„¸ì…˜ ìƒì„±"""
        if commodity in self.sessions:
            return self.sessions[commodity]

        s3 = self._get_s3_client()
        bucket = settings.model_s3_bucket

        cache_dir = self._cache_dir / commodity
        cache_dir.mkdir(parents=True, exist_ok=True)

        # S3ì—ì„œ ìµœì‹  íŒŒì¼ í‚¤ ì°¾ê¸°
        latest_onnx_key, latest_pkl_key = self._find_latest_s3_keys(s3, bucket)

        if not latest_onnx_key:
            raise FileNotFoundError(
                f"S3ì—ì„œ ONNX ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: "
                f"s3://{bucket}/{settings.model_s3_prefix}/"
            )

        # --- ONNX ë‹¤ìš´ë¡œë“œ ---
        model_local = cache_dir / "model.onnx"
        model_etag = self._download_if_changed(
            s3, bucket, latest_onnx_key, model_local,
            self._etags.get(commodity, {}).get("model"),
        )

        # --- PKL ë‹¤ìš´ë¡œë“œ ---
        pkl_local = cache_dir / "preprocessing.pkl"
        pkl_etag = None
        if latest_pkl_key:
            pkl_etag = self._download_if_changed(
                s3, bucket, latest_pkl_key, pkl_local,
                self._etags.get(commodity, {}).get("pkl"),
            )

        # ìºì‹œ ì—…ë°ì´íŠ¸
        self._etags[commodity] = {"model": model_etag, "pkl": pkl_etag}
        self._loaded_keys[commodity] = {
            "onnx_key": latest_onnx_key,
            "pkl_key": latest_pkl_key,
        }

        # ONNX ì„¸ì…˜ ìƒì„±
        logger.info(f"[{commodity}] ONNX ì„¸ì…˜ ìƒì„± ì¤‘...")
        session = ort.InferenceSession(
            str(model_local), providers=["CPUExecutionProvider"]
        )
        self.sessions[commodity] = session
        logger.info(f"âœ… [{commodity}] ONNX ì„¸ì…˜ ìƒì„± ì™„ë£Œ (from {latest_onnx_key})")

        # ì „ì²˜ë¦¬ ì •ë³´ ë¡œë“œ
        if pkl_local.exists() and pkl_local.stat().st_size > 0:
            with open(pkl_local, "rb") as f:
                self.preprocessing_info[commodity] = pickle.load(f)
            logger.info(f"âœ… [{commodity}] ì „ì²˜ë¦¬ ì •ë³´ ë¡œë“œ ì™„ë£Œ (from {latest_pkl_key})")
        else:
            logger.warning(f"âš ï¸ [{commodity}] ì „ì²˜ë¦¬ ì •ë³´ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤")

        return session

    def _find_latest_s3_keys(
        self, s3, bucket: str
    ) -> Tuple[Optional[str], Optional[str]]:
        """
        S3 prefix ì•„ë˜ íŒŒì¼ ëª©ë¡ì„ ì¡°íšŒí•˜ì—¬ ë‚ ì§œ(YYYYMMDD)ê°€ ê°€ì¥ í°
        ONNX / PKL íŒŒì¼ í‚¤ë¥¼ ë°˜í™˜.

        Returns:
            (latest_onnx_key, latest_pkl_key)  â€” ì—†ìœ¼ë©´ None
        """
        prefix = settings.model_s3_prefix.rstrip("/") + "/"

        paginator = s3.get_paginator("list_objects_v2")
        pages = paginator.paginate(Bucket=bucket, Prefix=prefix)

        onnx_candidates: list[Tuple[str, str]] = []  # (date_str, key)
        pkl_candidates: list[Tuple[str, str]] = []

        for page in pages:
            for obj in page.get("Contents", []):
                key = obj["Key"]
                filename = key.rsplit("/", 1)[-1]

                m_onnx = _ONNX_PATTERN.search(filename)
                if m_onnx:
                    onnx_candidates.append((m_onnx.group(1), key))
                    continue

                m_pkl = _PKL_PATTERN.search(filename)
                if m_pkl:
                    pkl_candidates.append((m_pkl.group(1), key))

        latest_onnx = max(onnx_candidates, key=lambda x: x[0])[1] if onnx_candidates else None
        latest_pkl = max(pkl_candidates, key=lambda x: x[0])[1] if pkl_candidates else None

        if latest_onnx:
            logger.info(f"S3 ìµœì‹  ONNX: {latest_onnx}")
        if latest_pkl:
            logger.info(f"S3 ìµœì‹  PKL:  {latest_pkl}")

        return latest_onnx, latest_pkl

    def _download_if_changed(
        self, s3, bucket: str, key: str, local_path: Path, cached_etag: Optional[str]
    ) -> Optional[str]:
        """
        S3 ì˜¤ë¸Œì íŠ¸ë¥¼ ETag ë¹„êµ í›„ ë³€ê²½ëì„ ë•Œë§Œ ë‹¤ìš´ë¡œë“œ.

        Returns:
            ìƒˆ ETag, ë˜ëŠ” None (íŒŒì¼ì´ S3ì— ì—†ì„ ë•Œ)
        """
        try:
            head = s3.head_object(Bucket=bucket, Key=key)
            remote_etag = head["ETag"]
        except Exception as e:
            logger.warning(f"S3 HEAD ì‹¤íŒ¨: s3://{bucket}/{key} â†’ {e}")
            return None

        if cached_etag == remote_etag and local_path.exists():
            logger.debug(f"ìºì‹œ ìœ íš¨ (ETag ë™ì¼): {key}")
            return remote_etag

        logger.info(f"S3 ë‹¤ìš´ë¡œë“œ: s3://{bucket}/{key} â†’ {local_path}")
        s3.download_file(bucket, key, str(local_path))
        logger.info(f"âœ… ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {local_path.name} ({head['ContentLength']} bytes)")

        return remote_etag

    def _get_s3_client(self):
        """boto3 S3 í´ë¼ì´ì–¸íŠ¸ (lazy init)"""
        if self._s3_client is None:
            import boto3

            self._s3_client = boto3.client(
                "s3",
                aws_access_key_id=settings.aws_access_key_id,
                aws_secret_access_key=settings.aws_secret_access_key,
                region_name=settings.aws_region,
            )
            logger.info(
                f"S3 í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”: region={settings.aws_region}, "
                f"bucket={settings.model_s3_bucket}"
            )

        return self._s3_client


# ===========================================
# ì‹±ê¸€í†¤ & ìŠ¤ì¼€ì¤„ëŸ¬
# ===========================================

_model_loader: Optional[ONNXModelLoader] = None


def get_model_loader() -> ONNXModelLoader:
    """ëª¨ë¸ ë¡œë” ì‹±ê¸€í†¤ ë°˜í™˜"""
    global _model_loader
    if _model_loader is None:
        _model_loader = ONNXModelLoader()
    return _model_loader


def start_model_update_scheduler():
    """
    ëª¨ë¸ ìë™ ì—…ë°ì´íŠ¸ ìŠ¤ì¼€ì¤„ëŸ¬ ì‹œì‘

    S3 ëª¨ë“œì—ì„œë§Œ ë™ì‘.
    MODEL_UPDATE_CHECK_TIME ì— ë”°ë¼ ë§¤ì¼ ì§€ì • ì‹œê°ì— S3ë¥¼ í™•ì¸í•˜ê³ ,
    ìƒˆ íŒŒì¼(ë‚ ì§œ suffixê°€ ë” í° íŒŒì¼)ì´ ìˆìœ¼ë©´ ìë™ ë¦¬ë¡œë“œ.
    """
    if settings.model_load_mode != "s3":
        logger.info("ë¡œì»¬ ëª¨ë“œ â†’ ëª¨ë¸ ì—…ë°ì´íŠ¸ ìŠ¤ì¼€ì¤„ëŸ¬ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
        return None

    from apscheduler.schedulers.background import BackgroundScheduler

    hour, minute = map(int, settings.model_update_check_time.split(":"))

    scheduler = BackgroundScheduler()

    def _check_update_job():
        logger.info("â° ìŠ¤ì¼€ì¤„ëŸ¬: ëª¨ë¸ ì—…ë°ì´íŠ¸ í™•ì¸ ì¤‘...")
        loader = get_model_loader()
        updated = loader.check_and_update("corn")
        if updated:
            logger.info("ğŸ”„ ëª¨ë¸ì´ ê°±ì‹ ë˜ì—ˆìŠµë‹ˆë‹¤.")
        else:
            logger.info("âœ… ëª¨ë¸ ë³€ê²½ ì—†ìŒ.")

    scheduler.add_job(
        _check_update_job,
        trigger="cron",
        hour=hour,
        minute=minute,
        id="model_update_check",
        replace_existing=True,
    )
    scheduler.start()
    logger.info(f"ğŸ“… ëª¨ë¸ ì—…ë°ì´íŠ¸ ìŠ¤ì¼€ì¤„ëŸ¬ ì‹œì‘: ë§¤ì¼ {settings.model_update_check_time}")

    return scheduler
